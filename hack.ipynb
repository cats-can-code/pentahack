{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Flashcard generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## App Diagram\n",
    "![diagram](diagram.png)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document OCR Processor / PIL, fitz:\n",
    "#### Uisng Document OCR Processor provided by google cloud, we are able to extract text blocks as well as images of the pdf. The images can also be extracted using PIL and fitz locally. the bounding boxes can be used to display boxes on the pdf in the app to allow the user to choose specific text to select for question generation.\n",
    "\n",
    "### Image classifier:\n",
    "#### Using the dataset provided by the ChartOCR GitHub for charts, a webscraper based on selenium for annotated diagrams and cifar10 for geenral images, we are able to train a CNN based classifier to separate the charts and annotated diagrams with pictures.\n",
    "\n",
    "### EasyOCR + Image Processing:\n",
    "#### Using the pretrained models included in EasyOCR, we are able to identify the text in the charts and annotated diagrams nd the corresponding bounding boxes to white them pt for fill in the blank questions.\n",
    "\n",
    "\n",
    "### ChatGPT:\n",
    "#### Using ChatGPT3.5-turbo's api, we are able to prompt the model to generate question and answer pairs based on the text provided. Chunking via NLTK is used to overcome the token limit, with 2000 tokens per chunk with 100 token overlaps. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "files = glob.glob('images/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "    \n",
    "    \n",
    "files = glob.glob('processed/*')\n",
    "for f in files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit this variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"pdf/NeRF_Navigation.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.api_core.client_options import ClientOptions\n",
    "from google.cloud import documentai_v1 as documentai\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PROJECT_ID = \"warm-alliance-382609\"\n",
    "LOCATION = \"us\"  # Format is 'us' or 'eu'\n",
    "PROCESSOR_ID = \"3b7728812751d25c\"  # Create processor in Cloud Console\n",
    "\n",
    "# The local file in your current working directory\n",
    "\n",
    "# Refer to https://cloud.google.com/document-ai/docs/file-types\n",
    "# for supported file types\n",
    "MIME_TYPE = \"application/pdf\"\n",
    "\n",
    "import os\n",
    "credential_path = r'C:\\Users\\Admin\\OneDrive\\Documents\\pentahack\\key.json'\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Instantiates a client\n",
    "docai_client = documentai.DocumentProcessorServiceClient(\n",
    "    client_options=ClientOptions(api_endpoint=f\"{LOCATION}-documentai.googleapis.com\")\n",
    ")\n",
    "\n",
    "# The full resource name of the processor, e.g.:\n",
    "# projects/project-id/locations/location/processor/processor-id\n",
    "# You must create new processors in the Cloud Console first\n",
    "RESOURCE_NAME = docai_client.processor_path(PROJECT_ID, LOCATION, PROCESSOR_ID)\n",
    "\n",
    "# Read the file into memory\n",
    "with open(FILE_PATH, \"rb\") as image:\n",
    "    image_content = image.read()\n",
    "\n",
    "# Load Binary Data into Document AI RawDocument Object\n",
    "raw_document = documentai.RawDocument(content=image_content, mime_type=MIME_TYPE)\n",
    "\n",
    "# Configure the process request\n",
    "request = documentai.ProcessRequest(name=RESOURCE_NAME, raw_document=raw_document)\n",
    "\n",
    "# Use the Document AI client to process the sample form\n",
    "result = docai_client.process_document(request=request)\n",
    "\n",
    "document_object = result.document\n",
    "\n",
    "\n",
    "\n",
    "print(\"Document processing complete.\")\n",
    "# print(f\"Text: {document_object.text}\")\n",
    "print(len(document_object.text))\n",
    "data = document_object.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8865\n",
      "Chunk 0: 2000 tokens\n",
      "Chunk 1: 2000 tokens\n",
      "Chunk 2: 2000 tokens\n",
      "Chunk 3: 2000 tokens\n",
      "Chunk 4: 1265 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(data)\n",
    "print(len(tokens))\n",
    "\n",
    "def break_up_file(tokens, chunk_size, overlap_size):\n",
    "    if len(tokens) <= chunk_size:\n",
    "        yield tokens\n",
    "    else:\n",
    "        chunk = tokens[:chunk_size]\n",
    "        yield chunk\n",
    "        yield from break_up_file(tokens[chunk_size-overlap_size:], chunk_size, overlap_size)\n",
    "\n",
    "def break_up_file_to_chunks(s, chunk_size=2000, overlap_size=100):\n",
    "    # with open(filename, 'r') as f:\n",
    "    #     text = f.read()\n",
    "    # tokens = word_tokenize(text)\n",
    "    return list(break_up_file(s, chunk_size, overlap_size))\n",
    "\n",
    "\n",
    "\n",
    "chunks = break_up_file_to_chunks(tokens)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i}: {len(chunk)} tokens\")\n",
    "    \n",
    "    \n",
    "\n",
    "def convert_to_detokenized_text(tokenized_text):\n",
    "    prompt_text = \" \".join(tokenized_text)\n",
    "    prompt_text = prompt_text.replace(\" 's\", \"'s\")\n",
    "    return prompt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What is a Neural Radiance Field (NeRF)?\n",
      "A: NeRF is a deep-learned geometric representation that encodes 3D geometry and color in a neural network, producing photo-realistic images through ray tracing.\n",
      "\n",
      "Q: How is NeRF different from other environment representations, like point clouds or voxel models?\n",
      "A: NeRF represents the environment as a continuous volumetric density and RGB values, trained on dense photographic images. It can efficiently store geometry as neural network weights, making it well-suited for memory constrained systems.\n",
      "\n",
      "Q: What is the objective of the proposed algorithm for navigating a robot through a NeRF environment?\n",
      "A: The objective is to navigate a robot through unoccupied space in the NeRF to reach a goal pose, using only an onboard RGB camera for localization.\n",
      "\n",
      "Q: How does the trajectory optimization algorithm avoid collisions with high-density regions in the NeRF?\n",
      "A: The trajectory optimization algorithm is based on a discrete time version of differential flatness, which plans full, dynamically feasible trajectories that avoid collisions with high-density regions of the NeRF.\n",
      "\n",
      "Q: How is the NeRF's image synthesis used to estimate the dynamic state of the robot in the environment?\n",
      "A: An optimization-based filter is used to estimate the dynamic state of the robot, balancing a dynamics prediction loss and a NeRF-based photometric loss, enabling the filter to estimate the robot's pose and velocity from an onboard RGB image.\n",
      "\n",
      "Q: What is the purpose of the online replanning loop in the navigation pipeline?\n",
      "A: The online replanning loop combines the trajectory planner and the filter in feedback to provide a full navigation pipeline for a robot to dynamically maneuver through a NeRF environment.\n",
      "\n",
      "Q: How does the proposed navigation pipeline compare to other vision-only navigation systems?\n",
      "A: The proposed navigation pipeline focuses on NeRFs as a geometric environment representation that enables any robot to navigate through it, differing from end-to-end approaches and specific training data and labels.\n",
      "\n",
      "Q: What are some advantages of using NeRFs as an environment representation for robots?\n",
      "A: NeRFs can compactly encode detailed 3D geometry and color, are efficient in memory-constrained systems, and can produce photo-realistic synthetic images. They can also handle transparent objects and render moving and deformable objects.\n",
      "\n",
      "Q: What simulators were used to present results in the paper?\n",
      "A: The proposed navigation pipeline was run with custom-trained NeRF models of a playground, a church, and Stonehenge in high fidelity simulation environments.\n",
      "\n",
      "Q: How was the performance of the trajectory planner and pose estimator evaluated?\n",
      "A: The performance was evaluated on the underlying ground truth mesh models, not the trained NeRF models, demonstrating robustness to model mismatch between the real-world scene and the trained NeRF.\n",
      "\n",
      "Q: What is a NeRF?\n",
      "A: A NeRF is a neural radiance field used to represent 3D environments with color and density information.\n",
      "\n",
      "Q: How does the NeRF-based collision metric work?\n",
      "A: The NeRF-based collision metric uses the probability of terminating a light ray as a proxy for collision probability, with the density output of the NeRF evaluated at the position of the robot body.\n",
      "\n",
      "Q: What does differential flatness refer to in the context of trajectory planning?\n",
      "A: Differential flatness refers to a property of some dynamical systems, such as quadrotors, where the system inputs and states can be represented using a smaller set of \"flat outputs\" and their derivatives.\n",
      "\n",
      "Q: How is the trajectory optimization problem formulated in this paper?\n",
      "A: The trajectory optimizer seeks a set of flat output waypoints that minimize a multi-objective cost function, which includes a collision penalty and a control penalty.\n",
      "\n",
      "Q: How is the state estimation filter formulated in this paper?\n",
      "A: The state estimation filter adds a process loss term to the photometric loss used in a previous method for estimating the robot's pose. It uses a recursive optimization approach to estimate the robot's pose and its derivatives.\n",
      "\n",
      "Q: How does the proposed pipeline use a NeRF representation of the environment?\n",
      "A: The proposed pipeline uses a NeRF representation of the environment for both planning and localization. The robot plans its path through the environment using the NeRF-based collision metric, and updates its belief about its state using the NeRF as a nonlinear measurement model.\n",
      "\n",
      "Q: What is the advantage of using differential flatness in trajectory planning?\n",
      "A: Differential flatness allows for a smaller set of \"flat outputs\" and their derivatives to be used to represent a dynamical system's inputs and states, which can speed up planning and make the optimization problem easier to solve.\n",
      "\n",
      "Q: How does the proposed method differ from traditional trajectory planning methods?\n",
      "A: The proposed method represents obstacles implicitly using the NeRF density, rather than in closed-form like traditional methods, and uses a denser set of waypoints whose location can be optimized directly.\n",
      "\n",
      "Q: How is the collision probability included in the trajectory optimization cost function?\n",
      "A: The collision probability is included in the cost function as a penalty term that depends on the NeRF density and the distance traveled by each point in the robot's point cloud.\n",
      "\n",
      "Q: What is the benefit of using a recursive SE(3) optimization for state estimation?\n",
      "A: Using a recursive optimization approach allows for the estimation of the robot's pose and its derivatives, which makes the state estimation more robust and useful for other robotics algorithms.\n",
      "\n",
      "Q: What is the objective of the proposed state estimator in the paper?\n",
      "A: The objective of the proposed state estimator is to estimate the robot's pose and its derivatives more robustly, especially in regions of low photometric gradient information.\n",
      "\n",
      "Q: How is the state estimation updated in the proposed filter?\n",
      "A: The state estimation is updated using the dynamics model and measurement data in the proposed filter.\n",
      "\n",
      "Q: What is the role of the NeRF model in the proposed online replanning pipeline?\n",
      "A: The NeRF model is used in the proposed online replanning pipeline to reason about collisions and plan dynamically feasible trajectories.\n",
      "\n",
      "Q: How does the proposed trajectory optimizer compare to minimum-snap trajectory planning in terms of optimization?\n",
      "A: The proposed trajectory optimizer is capable of optimizing the locations of waypoints based on the NeRF, while minimum-snap trajectory planning typically uses hand-placed waypoints.\n",
      "\n",
      "Q: What is the advantage of recursive SE (3) gradient descent over the method used in prior work?\n",
      "A: Recursive SE (3) gradient descent converges quicker and more smoothly than the method used in prior work, which is attributed to the noisy photometric loss landscape over the SE (3) manifold.\n",
      "\n",
      "Q: How is the covariance of the posterior determined in the proposed state estimator?\n",
      "A: The covariance of the posterior is determined using the known relationship between the Hessian of a Gaussian loss function and the covariance.\n",
      "\n",
      "Q: How does the proposed online replanning pipeline account for disturbances during execution?\n",
      "A: The proposed online replanning pipeline takes in new sensor data and updates its belief about the robot's state, and the new mean is used as a starting state for re-optimizing the trajectory.\n",
      "\n",
      "Q: What is the purpose of the subset of pixels J in the proposed state estimator?\n",
      "A: The subset of pixels J is selected to bias the sampling around areas of higher gradient information and identify points of interest for evaluation.\n",
      "\n",
      "Q: What type of robot is used to demonstrate the proposed planner in the paper?\n",
      "A: The proposed planner is demonstrated using both a quadrotor and an omnidirectional, couch-shaped mobility robot.\n",
      "\n",
      "Q: How is the collision loss objective in the proposed trajectory optimizer evaluated?\n",
      "A: The collision loss objective in the proposed trajectory optimizer is evaluated by comparing the NeRF predicted collisions with the ground truth mesh intersecting volume during trajectory planning.\n",
      "\n",
      "Q: What is the main advantage of the proposed trajectory planner?\n",
      "A: The proposed trajectory planner allows robots to navigate collision-free through the use of NeRF representations of the environment. \n",
      "\n",
      "Q: How does the proposed filter improve state estimation compared to a dynamically-initialized iNeRF estimator?\n",
      "A: The proposed filter outperforms a dynamically-initialized iNeRF estimator in rotational, translational, and velocity estimates while also sporting lower variance. \n",
      "\n",
      "Q: What is the size of the drone used in the evaluation of the proposed methods?\n",
      "A: The drone used in the evaluation is 0.5cm³ in volume. \n",
      "\n",
      "Q: What is the purpose of online replanning in the proposed pipeline?\n",
      "A: Online replanning allows the robot to adapt to unexpected changes and continue to reach its goal through the generation of collision-free trajectories. \n",
      "\n",
      "Q: How long does it typically take for the initial trajectory to be optimized?\n",
      "A: The initial trajectory typically requires 20 seconds over 2500 iterations to optimize. \n",
      "\n",
      "Q: What is the proposed method for utilizing multiple NeRFs to represent scenes with movable objects?\n",
      "A: The proposed method for utilizing multiple NeRFs is not mentioned in the given passage. \n",
      "\n",
      "Q: What is the proposed method for reducing collision risk based on uncertainty metrics calculated by the state estimator?\n",
      "A: The proposed method for reducing collision risk based on uncertainty metrics is mentioned as a possible direction for future work. \n",
      "\n",
      "Q: What is the purpose of the proposed perception-control integration?\n",
      "A: The proposed perception-control integration seeks to improve navigation by encouraging trajectories to point the camera in directions with greater gradient information. \n",
      "\n",
      "Q: What is the proposed method for improving execution speed of the algorithm?\n",
      "A: The proposed method for improving execution speed is to leverage improvements in the underlying NeRF representation. \n",
      "\n",
      "Q: What type of robot is the proposed method implemented on in the future?\n",
      "A: The proposed method is planned to be implemented on quadrotors in real scenes to demonstrate performance beyond simulation.\n",
      "\n",
      "Q: What is PixelNeRF?\n",
      "A: PixelNeRF is a neural radiance field method used for generating high-resolution 3D images from only one or few 2D images.\n",
      "\n",
      "Q: What is FastNeRF?\n",
      "A: FastNeRF is a high-fidelity neural rendering method that can produce 200 frames per second.\n",
      "\n",
      "Q: What are Implicit Surface Representations?\n",
      "A: Implicit Surface Representations are neural network layers used for 3D shape representation.\n",
      "\n",
      "Q: What is DeepSDF?\n",
      "A: DeepSDF is a method used for learning continuous signed distance functions for shape representation.\n",
      "\n",
      "Q: What are Implicit Neural Representations with Periodic Activation Functions?\n",
      "A: Implicit Neural Representations with Periodic Activation Functions are a type of neural representation method used for generating implicit surfaces with a periodic activation function.\n",
      "\n",
      "Q: What are Occupancy Networks?\n",
      "A: Occupancy Networks are a type of neural network used for learning 3D reconstruction in function space.\n",
      "\n",
      "Q: What is iNeRF?\n",
      "A: iNeRF, or inverting neural radiance fields, is a method used for pose estimation.\n",
      "\n",
      "Q: What is the purpose of trajectory optimization?\n",
      "A: The purpose of trajectory optimization is to calculate the optimal path for a moving object while avoiding obstacles and adhering to constraints.\n",
      "\n",
      "Q: What is the Minimum Snap Trajectory Generation method?\n",
      "A: The Minimum Snap Trajectory Generation method is a control method used for generating the optimal trajectory of quadrotors.\n",
      "\n",
      "Q: What are the Schwartz and Sharir Piano Movers Problems?\n",
      "A: The Schwartz and Sharir Piano Movers Problems are mathematical problems that involve moving a two-dimensional rigid polygonal body amidst polygonal barriers.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "qa = ''\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "  completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "      {\"role\": \"user\", \"content\": convert_to_detokenized_text(chunks[i]) + \" \\n Using only the information provided above and not using the general knowledge of chatgpt, make 10 unique and truthful question and answer pairs using the format:\\nQ:\\nA:\"}\n",
    "    ]\n",
    "  )\n",
    "\n",
    "  print('\\n' + completion.choices[0].message.content)\n",
    "  qa += completion.choices[0].message.content + '\\n'\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 1\n",
      "Q: What is a Neural Radiance Field (NeRF)?\n",
      "A: NeRF is a deep-learned geometric representation that encodes 3D geometry and color in a neural network, producing photo-realistic images through ray tracing.\n",
      "Question: 2\n",
      "Q: How is NeRF different from other environment representations, like point clouds or voxel models?\n",
      "A: NeRF represents the environment as a continuous volumetric density and RGB values, trained on dense photographic images. It can efficiently store geometry as neural network weights, making it well-suited for memory constrained systems.\n",
      "Question: 3\n",
      "Q: What is the objective of the proposed algorithm for navigating a robot through a NeRF environment?\n",
      "A: The objective is to navigate a robot through unoccupied space in the NeRF to reach a goal pose, using only an onboard RGB camera for localization.\n",
      "Question: 4\n",
      "Q: How does the trajectory optimization algorithm avoid collisions with high-density regions in the NeRF?\n",
      "A: The trajectory optimization algorithm is based on a discrete time version of differential flatness, which plans full, dynamically feasible trajectories that avoid collisions with high-density regions of the NeRF.\n",
      "Question: 5\n",
      "Q: How is the NeRF's image synthesis used to estimate the dynamic state of the robot in the environment?\n",
      "A: An optimization-based filter is used to estimate the dynamic state of the robot, balancing a dynamics prediction loss and a NeRF-based photometric loss, enabling the filter to estimate the robot's pose and velocity from an onboard RGB image.\n",
      "Question: 6\n",
      "Q: What is the purpose of the online replanning loop in the navigation pipeline?\n",
      "A: The online replanning loop combines the trajectory planner and the filter in feedback to provide a full navigation pipeline for a robot to dynamically maneuver through a NeRF environment.\n",
      "Question: 7\n",
      "Q: How does the proposed navigation pipeline compare to other vision-only navigation systems?\n",
      "A: The proposed navigation pipeline focuses on NeRFs as a geometric environment representation that enables any robot to navigate through it, differing from end-to-end approaches and specific training data and labels.\n",
      "Question: 8\n",
      "Q: What are some advantages of using NeRFs as an environment representation for robots?\n",
      "A: NeRFs can compactly encode detailed 3D geometry and color, are efficient in memory-constrained systems, and can produce photo-realistic synthetic images. They can also handle transparent objects and render moving and deformable objects.\n",
      "Question: 9\n",
      "Q: What simulators were used to present results in the paper?\n",
      "A: The proposed navigation pipeline was run with custom-trained NeRF models of a playground, a church, and Stonehenge in high fidelity simulation environments.\n",
      "Question: 10\n",
      "Q: How was the performance of the trajectory planner and pose estimator evaluated?\n",
      "A: The performance was evaluated on the underlying ground truth mesh models, not the trained NeRF models, demonstrating robustness to model mismatch between the real-world scene and the trained NeRF.\n",
      "Question: 11\n",
      "Q: What is a NeRF?\n",
      "A: A NeRF is a neural radiance field used to represent 3D environments with color and density information.\n",
      "Question: 12\n",
      "Q: How does the NeRF-based collision metric work?\n",
      "A: The NeRF-based collision metric uses the probability of terminating a light ray as a proxy for collision probability, with the density output of the NeRF evaluated at the position of the robot body.\n",
      "Question: 13\n",
      "Q: What does differential flatness refer to in the context of trajectory planning?\n",
      "A: Differential flatness refers to a property of some dynamical systems, such as quadrotors, where the system inputs and states can be represented using a smaller set of \"flat outputs\" and their derivatives.\n",
      "Question: 14\n",
      "Q: How is the trajectory optimization problem formulated in this paper?\n",
      "A: The trajectory optimizer seeks a set of flat output waypoints that minimize a multi-objective cost function, which includes a collision penalty and a control penalty.\n",
      "Question: 15\n",
      "Q: How is the state estimation filter formulated in this paper?\n",
      "A: The state estimation filter adds a process loss term to the photometric loss used in a previous method for estimating the robot's pose. It uses a recursive optimization approach to estimate the robot's pose and its derivatives.\n",
      "Question: 16\n",
      "Q: How does the proposed pipeline use a NeRF representation of the environment?\n",
      "A: The proposed pipeline uses a NeRF representation of the environment for both planning and localization. The robot plans its path through the environment using the NeRF-based collision metric, and updates its belief about its state using the NeRF as a nonlinear measurement model.\n",
      "Question: 17\n",
      "Q: What is the advantage of using differential flatness in trajectory planning?\n",
      "A: Differential flatness allows for a smaller set of \"flat outputs\" and their derivatives to be used to represent a dynamical system's inputs and states, which can speed up planning and make the optimization problem easier to solve.\n",
      "Question: 18\n",
      "Q: How does the proposed method differ from traditional trajectory planning methods?\n",
      "A: The proposed method represents obstacles implicitly using the NeRF density, rather than in closed-form like traditional methods, and uses a denser set of waypoints whose location can be optimized directly.\n",
      "Question: 19\n",
      "Q: How is the collision probability included in the trajectory optimization cost function?\n",
      "A: The collision probability is included in the cost function as a penalty term that depends on the NeRF density and the distance traveled by each point in the robot's point cloud.\n",
      "Question: 20\n",
      "Q: What is the benefit of using a recursive SE(3) optimization for state estimation?\n",
      "A: Using a recursive optimization approach allows for the estimation of the robot's pose and its derivatives, which makes the state estimation more robust and useful for other robotics algorithms.\n",
      "Question: 21\n",
      "Q: What is the objective of the proposed state estimator in the paper?\n",
      "A: The objective of the proposed state estimator is to estimate the robot's pose and its derivatives more robustly, especially in regions of low photometric gradient information.\n",
      "Question: 22\n",
      "Q: How is the state estimation updated in the proposed filter?\n",
      "A: The state estimation is updated using the dynamics model and measurement data in the proposed filter.\n",
      "Question: 23\n",
      "Q: What is the role of the NeRF model in the proposed online replanning pipeline?\n",
      "A: The NeRF model is used in the proposed online replanning pipeline to reason about collisions and plan dynamically feasible trajectories.\n",
      "Question: 24\n",
      "Q: How does the proposed trajectory optimizer compare to minimum-snap trajectory planning in terms of optimization?\n",
      "A: The proposed trajectory optimizer is capable of optimizing the locations of waypoints based on the NeRF, while minimum-snap trajectory planning typically uses hand-placed waypoints.\n",
      "Question: 25\n",
      "Q: What is the advantage of recursive SE (3) gradient descent over the method used in prior work?\n",
      "A: Recursive SE (3) gradient descent converges quicker and more smoothly than the method used in prior work, which is attributed to the noisy photometric loss landscape over the SE (3) manifold.\n",
      "Question: 26\n",
      "Q: How is the covariance of the posterior determined in the proposed state estimator?\n",
      "A: The covariance of the posterior is determined using the known relationship between the Hessian of a Gaussian loss function and the covariance.\n",
      "Question: 27\n",
      "Q: How does the proposed online replanning pipeline account for disturbances during execution?\n",
      "A: The proposed online replanning pipeline takes in new sensor data and updates its belief about the robot's state, and the new mean is used as a starting state for re-optimizing the trajectory.\n",
      "Question: 28\n",
      "Q: What is the purpose of the subset of pixels J in the proposed state estimator?\n",
      "A: The subset of pixels J is selected to bias the sampling around areas of higher gradient information and identify points of interest for evaluation.\n",
      "Question: 29\n",
      "Q: What type of robot is used to demonstrate the proposed planner in the paper?\n",
      "A: The proposed planner is demonstrated using both a quadrotor and an omnidirectional, couch-shaped mobility robot.\n",
      "Question: 30\n",
      "Q: How is the collision loss objective in the proposed trajectory optimizer evaluated?\n",
      "A: The collision loss objective in the proposed trajectory optimizer is evaluated by comparing the NeRF predicted collisions with the ground truth mesh intersecting volume during trajectory planning.\n",
      "Question: 31\n",
      "Q: What is the main advantage of the proposed trajectory planner?\n",
      "A: The proposed trajectory planner allows robots to navigate collision-free through the use of NeRF representations of the environment. \n",
      "Question: 32\n",
      "Q: How does the proposed filter improve state estimation compared to a dynamically-initialized iNeRF estimator?\n",
      "A: The proposed filter outperforms a dynamically-initialized iNeRF estimator in rotational, translational, and velocity estimates while also sporting lower variance. \n",
      "Question: 33\n",
      "Q: What is the size of the drone used in the evaluation of the proposed methods?\n",
      "A: The drone used in the evaluation is 0.5cm³ in volume. \n",
      "Question: 34\n",
      "Q: What is the purpose of online replanning in the proposed pipeline?\n",
      "A: Online replanning allows the robot to adapt to unexpected changes and continue to reach its goal through the generation of collision-free trajectories. \n",
      "Question: 35\n",
      "Q: How long does it typically take for the initial trajectory to be optimized?\n",
      "A: The initial trajectory typically requires 20 seconds over 2500 iterations to optimize. \n",
      "Question: 36\n",
      "Q: What is the proposed method for utilizing multiple NeRFs to represent scenes with movable objects?\n",
      "A: The proposed method for utilizing multiple NeRFs is not mentioned in the given passage. \n",
      "Question: 37\n",
      "Q: What is the proposed method for reducing collision risk based on uncertainty metrics calculated by the state estimator?\n",
      "A: The proposed method for reducing collision risk based on uncertainty metrics is mentioned as a possible direction for future work. \n",
      "Question: 38\n",
      "Q: What is the purpose of the proposed perception-control integration?\n",
      "A: The proposed perception-control integration seeks to improve navigation by encouraging trajectories to point the camera in directions with greater gradient information. \n",
      "Question: 39\n",
      "Q: What is the proposed method for improving execution speed of the algorithm?\n",
      "A: The proposed method for improving execution speed is to leverage improvements in the underlying NeRF representation. \n",
      "Question: 40\n",
      "Q: What type of robot is the proposed method implemented on in the future?\n",
      "A: The proposed method is planned to be implemented on quadrotors in real scenes to demonstrate performance beyond simulation.\n",
      "Question: 41\n",
      "Q: What is PixelNeRF?\n",
      "A: PixelNeRF is a neural radiance field method used for generating high-resolution 3D images from only one or few 2D images.\n",
      "Question: 42\n",
      "Q: What is FastNeRF?\n",
      "A: FastNeRF is a high-fidelity neural rendering method that can produce 200 frames per second.\n",
      "Question: 43\n",
      "Q: What are Implicit Surface Representations?\n",
      "A: Implicit Surface Representations are neural network layers used for 3D shape representation.\n",
      "Question: 44\n",
      "Q: What is DeepSDF?\n",
      "A: DeepSDF is a method used for learning continuous signed distance functions for shape representation.\n",
      "Question: 45\n",
      "Q: What are Implicit Neural Representations with Periodic Activation Functions?\n",
      "A: Implicit Neural Representations with Periodic Activation Functions are a type of neural representation method used for generating implicit surfaces with a periodic activation function.\n",
      "Question: 46\n",
      "Q: What are Occupancy Networks?\n",
      "A: Occupancy Networks are a type of neural network used for learning 3D reconstruction in function space.\n",
      "Question: 47\n",
      "Q: What is iNeRF?\n",
      "A: iNeRF, or inverting neural radiance fields, is a method used for pose estimation.\n",
      "Question: 48\n",
      "Q: What is the purpose of trajectory optimization?\n",
      "A: The purpose of trajectory optimization is to calculate the optimal path for a moving object while avoiding obstacles and adhering to constraints.\n",
      "Question: 49\n",
      "Q: What is the Minimum Snap Trajectory Generation method?\n",
      "A: The Minimum Snap Trajectory Generation method is a control method used for generating the optimal trajectory of quadrotors.\n",
      "Question: 50\n",
      "Q: What are the Schwartz and Sharir Piano Movers Problems?\n",
      "A: The Schwartz and Sharir Piano Movers Problems are mathematical problems that involve moving a two-dimensional rigid polygonal body amidst polygonal barriers.\n"
     ]
    }
   ],
   "source": [
    "def remove_suffix(input_string, suffix):\n",
    "    if suffix and input_string.endswith(suffix):\n",
    "        return input_string[:-len(suffix)]\n",
    "    return input_string\n",
    "\n",
    "with  open('qa.txt' , 'w+') as f:\n",
    "    f.write(qa)\n",
    "    f.close()\n",
    "\n",
    "q , a = [] , []\n",
    "    \n",
    "with open('qa.txt' , 'r') as file:\n",
    "    for line in file:\n",
    "        if line[0] == 'Q': q.append(remove_suffix(line, '\\n'))\n",
    "        if line[0] == 'A': a.append(remove_suffix(line, '\\n'))\n",
    "        \n",
    "# print(q)\n",
    "# print(a)\n",
    "\n",
    "pairs = list(zip(q,a))\n",
    "# print(pairs)\n",
    "for i in range(len(pairs)):\n",
    "    print('Question: '+  str(i + 1))\n",
    "    print(pairs[i][0])\n",
    "    print(pairs[i][1])\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymupdf in c:\\users\\admin\\onedrive\\documents\\anaconda3\\envs\\penta\\lib\\site-packages (1.21.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    }
   ],
   "source": [
    "pip install pymupdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'fitz' has no attribute 'open'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m file \u001b[39m=\u001b[39m FILE_PATH\n\u001b[0;32m      6\u001b[0m \u001b[39m# open the file\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m pdf_file \u001b[39m=\u001b[39m fitz\u001b[39m.\u001b[39;49mopen(file)\n\u001b[0;32m      9\u001b[0m \u001b[39m# iterate over pdf pages\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m page_index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(pdf_file)):\n\u001b[0;32m     11\u001b[0m     \u001b[39m# get the page itself\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'fitz' has no attribute 'open'"
     ]
    }
   ],
   "source": [
    "\n",
    "import io\n",
    "import fitz\n",
    "from PIL import Image\n",
    "\n",
    "file = FILE_PATH\n",
    "# open the file\n",
    "pdf_file = fitz.open(file)\n",
    "\n",
    "# iterate over pdf pages\n",
    "for page_index in range(len(pdf_file)):\n",
    "    # get the page itself\n",
    "    page = pdf_file[page_index]\n",
    "    image_list = page.get_images()\n",
    "    # printing number of images found in this page\n",
    "    if image_list:\n",
    "        print(f\"[+] Found a total of {len(image_list)} images in page {page_index}\")\n",
    "    else:\n",
    "        print(\"[!] No images found on page\", page_index)\n",
    "    for image_index, img in enumerate(page.get_images(), start=1):\n",
    "        # get the XREF of the image\n",
    "        xref = img[0]\n",
    "        # extract the image bytes\n",
    "        base_image = pdf_file.extract_image(xref)\n",
    "        image_bytes = base_image[\"image\"]\n",
    "        # get the image extension\n",
    "        image_ext = base_image[\"ext\"]\n",
    "        # load it to PIL\n",
    "        image = Image.open(io.BytesIO(image_bytes))\n",
    "        # save it to local disk\n",
    "        image.save(open(f\"images/image{page_index+1}_{image_index}.{image_ext}\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def preprocess_bytes(byte_string):\n",
    "    with open(byte_string, 'rb') as f:\n",
    "        img_array = np.asarray(bytearray(f.read()), dtype=\"uint8\")\n",
    "        img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
    "        img_resized = cv2.resize(img, dsize=(32, 32), interpolation=cv2.INTER_CUBIC)\n",
    "        return img_resized\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "img = tf.data.Dataset.list_files('images/*.png')\n",
    "\n",
    "\n",
    "img_arr = []\n",
    "\n",
    "img_iterator = img.as_numpy_iterator()\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        img_arr.append(preprocess_bytes(img_iterator.next()))\n",
    "    except:\n",
    "        break\n",
    "\n",
    "img_arr = np.array(img_arr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step\n",
      "['image1_1.jpeg', 'image3_1.png', 'image4_1.png', 'image4_2.png', 'image5_1.png', 'image6_1.jpeg', 'image6_2.png', 'image6_3.png', 'image6_4.png', 'image6_5.jpeg', 'image7_1.png', 'image7_2.png', 'image7_3.jpeg']\n",
      "9\n",
      "['image1_1.jpeg', 'image3_1.png', 'image4_1.png', 'image4_2.png', 'image5_1.png', 'image6_1.jpeg', 'image6_2.png', 'image6_3.png', 'image6_4.png']\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "file_path = f\"./modelsave/model2.h5\"\n",
    "model = keras.models.load_model(file_path)\n",
    "pred = model.predict(img_arr)\n",
    "\n",
    "ans = [[0,0,0]]*len(pred)\n",
    "for q in range(len(pred)):\n",
    "    for a in range(len(pred[q])):\n",
    "        # print(pred[q][a]/1 > 0.8)\n",
    "        if pred[q][a]/1 > 0.8: ans[q][a] =1\n",
    "        else: ans[q][a] = 0\n",
    "        \n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyfiles = [f for f in listdir('images/')]\n",
    "print(onlyfiles)\n",
    "filtered = []\n",
    "idx = 0\n",
    "for a,b,c in ans:\n",
    "    if a  == 1 or b == 1: filtered.append(onlyfiles[idx])\n",
    "    idx +=1\n",
    "    \n",
    "print(len(filtered))\n",
    "print(filtered)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA not available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image1_1.jpeg\n",
      "image3_1.png\n",
      "image4_1.png\n",
      "image4_2.png\n",
      "image5_1.png\n",
      "image6_1.jpeg\n",
      "image6_2.png\n",
      "image6_3.png\n",
      "image6_4.png\n",
      "image6_5.jpeg\n",
      "image7_1.png\n",
      "image7_2.png\n",
      "image7_3.jpeg\n",
      "['processed/rectanglebox-{i}.jpg', 'processed/rectanglebox-{i}.jpg', 'processed/rectanglebox-{i}.jpg', 'processed/rectanglebox-{i}.jpg', 'processed/rectanglebox-{i}.jpg', 'processed/rectanglebox-{i}.jpg', 'processed/rectanglebox-{i}.jpg']\n"
     ]
    }
   ],
   "source": [
    "import cv2 \n",
    "import easyocr\n",
    "from datetime import datetime\n",
    "\n",
    "reader = easyocr.Reader(['en'], gpu = True)\n",
    "\n",
    "\n",
    "names = []\n",
    "for i in onlyfiles:\n",
    "    boxes = reader.readtext(f'images/{i}')\n",
    "    img = cv2.imread(f\"images/{i}\")\n",
    "    im2 = img.copy()\n",
    "    print(i)\n",
    "    for box in boxes:\n",
    "        x, y, w, h = int(box[0][0][0]), int(box[0][0][1]), int(box[0][1][0] - box[0][0][0]), int(box[0][0][1] - box[0][2][1]) \n",
    "        \n",
    "        # Draw the bounding box on the text area\n",
    "        rect=cv2.rectangle(im2, (x, y), (x + w, y - h), (0, 255, 0), 2)\n",
    "        \n",
    "        # Crop the bounding box area\n",
    "        cropped = im2[y:y - h, x:x + w]\n",
    "        \n",
    "        cv2.imwrite(f'processed/rectanglebox-{i}.jpg',rect)\n",
    "    if len(boxes) > 0: names.append('processed/rectanglebox-{i}.jpg')\n",
    "print(names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
