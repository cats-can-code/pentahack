{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Flashcard generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## App Diagram\n",
    "![diagram](diagram.png)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document OCR Processor / PIL, fitz:\n",
    "#### Uisng Document OCR Processor provided by google cloud, we are able to extract text blocks as well as images of the pdf. The images can also be extracted using PIL and fitz locally. the bounding boxes can be used to display boxes on the pdf in the app to allow the user to choose specific text to select for question generation.\n",
    "\n",
    "### Image classifier:\n",
    "#### Using the dataset provided by the ChartOCR GitHub for charts, a webscraper based on selenium for annotated diagrams and cifar10 for geenral images, we are able to train a CNN based classifier to separate the charts and annotated diagrams with pictures.\n",
    "\n",
    "### EasyOCR + Image Processing:\n",
    "#### Using the pretrained models included in EasyOCR, we are able to identify the text in the charts and annotated diagrams nd the corresponding bounding boxes to white them pt for fill in the blank questions.\n",
    "\n",
    "\n",
    "### ChatGPT:\n",
    "#### Using ChatGPT3.5-turbo's api, we are able to prompt the model to generate question and answer pairs based on the text provided. Chunking via NLTK is used to overcome the token limit, with 2000 tokens per chunk with 100 token overlaps. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "files = glob.glob('images/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "    \n",
    "    \n",
    "files = glob.glob('processed/*')\n",
    "for f in files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document processing complete.\n",
      "46147\n"
     ]
    }
   ],
   "source": [
    "from google.api_core.client_options import ClientOptions\n",
    "from google.cloud import documentai_v1 as documentai\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PROJECT_ID = \"warm-alliance-382609\"\n",
    "LOCATION = \"us\"  # Format is 'us' or 'eu'\n",
    "PROCESSOR_ID = \"3b7728812751d25c\"  # Create processor in Cloud Console\n",
    "\n",
    "# The local file in your current working directory\n",
    "FILE_PATH = \"pdf/NeRF_Navigation.pdf\"\n",
    "# Refer to https://cloud.google.com/document-ai/docs/file-types\n",
    "# for supported file types\n",
    "MIME_TYPE = \"application/pdf\"\n",
    "\n",
    "import os\n",
    "credential_path = r'C:\\Users\\Admin\\OneDrive\\Documents\\pentahack\\key.json'\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Instantiates a client\n",
    "docai_client = documentai.DocumentProcessorServiceClient(\n",
    "    client_options=ClientOptions(api_endpoint=f\"{LOCATION}-documentai.googleapis.com\")\n",
    ")\n",
    "\n",
    "# The full resource name of the processor, e.g.:\n",
    "# projects/project-id/locations/location/processor/processor-id\n",
    "# You must create new processors in the Cloud Console first\n",
    "RESOURCE_NAME = docai_client.processor_path(PROJECT_ID, LOCATION, PROCESSOR_ID)\n",
    "\n",
    "# Read the file into memory\n",
    "with open(FILE_PATH, \"rb\") as image:\n",
    "    image_content = image.read()\n",
    "\n",
    "# Load Binary Data into Document AI RawDocument Object\n",
    "raw_document = documentai.RawDocument(content=image_content, mime_type=MIME_TYPE)\n",
    "\n",
    "# Configure the process request\n",
    "request = documentai.ProcessRequest(name=RESOURCE_NAME, raw_document=raw_document)\n",
    "\n",
    "# Use the Document AI client to process the sample form\n",
    "result = docai_client.process_document(request=request)\n",
    "\n",
    "document_object = result.document\n",
    "\n",
    "\n",
    "\n",
    "print(\"Document processing complete.\")\n",
    "# print(f\"Text: {document_object.text}\")\n",
    "print(len(document_object.text))\n",
    "data = document_object.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8865\n",
      "Chunk 0: 2000 tokens\n",
      "Chunk 1: 2000 tokens\n",
      "Chunk 2: 2000 tokens\n",
      "Chunk 3: 2000 tokens\n",
      "Chunk 4: 1265 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(data)\n",
    "print(len(tokens))\n",
    "\n",
    "def break_up_file(tokens, chunk_size, overlap_size):\n",
    "    if len(tokens) <= chunk_size:\n",
    "        yield tokens\n",
    "    else:\n",
    "        chunk = tokens[:chunk_size]\n",
    "        yield chunk\n",
    "        yield from break_up_file(tokens[chunk_size-overlap_size:], chunk_size, overlap_size)\n",
    "\n",
    "def break_up_file_to_chunks(s, chunk_size=2000, overlap_size=100):\n",
    "    # with open(filename, 'r') as f:\n",
    "    #     text = f.read()\n",
    "    # tokens = word_tokenize(text)\n",
    "    return list(break_up_file(s, chunk_size, overlap_size))\n",
    "\n",
    "\n",
    "\n",
    "chunks = break_up_file_to_chunks(tokens)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i}: {len(chunk)} tokens\")\n",
    "    \n",
    "    \n",
    "\n",
    "def convert_to_detokenized_text(tokenized_text):\n",
    "    prompt_text = \" \".join(tokenized_text)\n",
    "    prompt_text = prompt_text.replace(\" 's\", \"'s\")\n",
    "    return prompt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What is a Neural Radiance Field (NeRF)?\n",
      "A: NeRFs are deep-learned geometric representations that encode 3D geometry and color, enabling the efficient and photo-realistic simulation of scenes and objects.\n",
      "\n",
      "Q: How can NeRFs be used for robot navigation?\n",
      "A: NeRFs can be used to represent the environment as a continuous density field, allowing for more efficient and gradient-based trajectory planning and motion optimization methods for robots using only RGB camera feedback.\n",
      "\n",
      "Q: What advantages do NeRFs have over traditional environment representations like voxel grids or point clouds?\n",
      "A: NeRFs are trained directly on photographic images, making them more robust to environmental changes and easier to construct without precise feature extraction or alignment. They also encode the geometry as a continuous density field, enabling more efficient motion planning algorithms.\n",
      "\n",
      "Q: How does the trajectory optimization algorithm work in this paper?\n",
      "A: The trajectory optimization algorithm is based on differential flatness and plans full, dynamically feasible trajectories that avoid collisions with high-density regions in the NeRF environment.\n",
      "\n",
      "Q: What is the online replanning controller?\n",
      "A: The online replanning controller combines the trajectory planner with an optimization-based filter to estimate the robot's full dynamic state, and is used to navigate the robot through the environment in real-time using only onboard RGB camera feedback.\n",
      "\n",
      "Q: What types of environments were used in the simulation results?\n",
      "A: The simulation results used custom-trained NeRF models of a playground, a church, and Stonehenge.\n",
      "\n",
      "Q: What is the purpose of the pose filter in the navigation pipeline?\n",
      "A: The pose filter is used to estimate the robot's dynamic state given only onboard RGB camera feedback, and is used to update the trajectory planner and optimize the robot's control inputs in real-time.\n",
      "\n",
      "Q: How is collision avoidance imposed in trajectory optimization for NeRF environments?\n",
      "A: Collision avoidance is imposed by constraining the SDF for all obstacles to be non-negative at all points on the robot body along the trajectory.\n",
      "\n",
      "Q: How does this work differ from other vision-only navigation systems?\n",
      "A: This work focuses on using NeRFs as a geometric environment representation that can be used by any robot for navigation, and uses a modular approach to separate perception and control. It also specifically addresses the problem of collision avoidance in trajectory planning for NeRF environments.\n",
      "\n",
      "Q: What is the main contribution of this paper?\n",
      "A: The main contribution of this paper is a vision-based navigation pipeline for a robot navigating through a 3D environment represented as a NeRF using only an onboard RGB camera for localization. This pipeline includes a new trajectory planning method, an optimization-based pose filter, and an online replanning controller.\n",
      "\n",
      "Q: What is the representation used for the environment in this paper's navigation method?\n",
      "A: The navigation method uses a NeRF (Neural Radiance Fields) representation for the environment.\n",
      "\n",
      "Q: What is the objective of the trajectory optimization in this paper's navigation method?\n",
      "A: The trajectory optimization seeks to minimize a combined objective of collision probability and control effort.\n",
      "\n",
      "Q: How does the method in this paper leverage differential flatness to speed up planning?\n",
      "A: The method uses differential flatness, a property of some dynamical systems which allows their inputs and states to be represented using a smaller set of \"flat outputs\" and their derivatives, to speed up planning.\n",
      "\n",
      "Q: What is the cost function used in the trajectory optimization of this paper's navigation method?\n",
      "A: The cost function used in the trajectory optimization seeks to minimize multi-objective cost given by collision penalty control penalty.\n",
      "\n",
      "Q: What is the difference between CHOMP and the trajectory optimizer used in this paper's navigation method?\n",
      "A: CHOMP represents obstacle geometry by precomputing each obstacle's Signed Distance Field (SDF) on a finite grid, while the trajectory optimizer in this paper's navigation method uses a NeRF geometry representation that is continuous and of arbitrary resolution.\n",
      "\n",
      "Q: What is the main challenge that prevents common trajectory planning methods from working with NeRF environment representations?\n",
      "A: Querying a NeRF at a point in space gives a density, not an absolute occupancy, which prevents the use of hard constraints.\n",
      "\n",
      "Q: What is the approach used in this paper's navigation method to deal with the challenge of using NeRF environment representations?\n",
      "A: The paper's navigation method represents obstacles implicitly using the NeRF density and uses a set of flat output waypoints that are optimized to minimize a combined objective of collision probability and control effort.\n",
      "\n",
      "Q: How is the method in this paper's navigation approach different from traditional planning pipelines for differentially flat systems?\n",
      "A: While prior methods only optimize the trajectory between static, hand-designed waypoints, this paper's navigation method uses a denser set of waypoints whose location can be optimized directly.\n",
      "\n",
      "Q: What is the purpose of the state estimator in the proposed pipeline in this paper's navigation method?\n",
      "A: The state estimator uses a NeRF as a nonlinear measurement model to update the robot's belief about its current state after it takes an action and receives a noisy image from its onboard camera.\n",
      "\n",
      "Q: What is the added benefit of the state estimation filter proposed in this paper's navigation method?\n",
      "A: The state estimation filter produces a state covariance which can be useful for other robotics algorithms running in parallel, such as collision avoidance with dynamic agents.\n",
      "\n",
      "Q: What is the purpose of the proposed trajectory planner?\n",
      "A: The proposed trajectory planner is designed to generate aggressive, dynamically feasible trajectories for a robot that can avoid collisions in various environments.\n",
      "\n",
      "Q: How does the state estimator improve the robot's performance in low photometric gradient regions?\n",
      "A: The state estimator relies more on the dynamics model to produce a more robust state estimation when the robot travels through areas with low photometric gradient information.\n",
      "\n",
      "Q: What is the cost function used in the state estimation process?\n",
      "A: The cost function used in the state estimation process is a combination of the photometric loss and process loss.\n",
      "\n",
      "Q: How does the algorithm reject outliers in the state estimation process?\n",
      "A: Outlier rejection is performed on the per-pixel loss to reduce variance in the state estimation process.\n",
      "\n",
      "Q: What is the role of the NeRF model in the online replanning pipeline?\n",
      "A: The NeRF model is used to help the robot reason about collisions and generate updated plans during the online replanning process.\n",
      "\n",
      "Q: How are trajectories optimized in the proposed trajectory planner?\n",
      "A: Trajectories are optimized using differential flatness to compute trajectories that pass through a series of waypoints, which are optimized based on the NeRF model.\n",
      "\n",
      "Q: What is the purpose of the minimum-snap planner in the comparison study?\n",
      "A: The minimum-snap planner is used as a baseline comparison to the proposed trajectory planner, as both use differential flatness to compute trajectories.\n",
      "\n",
      "Q: How is the performance of the proposed trajectory planner evaluated in the comparison study?\n",
      "A: The performance of the proposed trajectory planner is evaluated against the minimum-snap planner and a Rapidly-exploring Random Trees (RRT) method on 10 trajectories with varying obstacles and complexities in the Stonehenge environment.\n",
      "\n",
      "Q: What is the difficulty presented in the omnidirectional robot scenario?\n",
      "A: The difficulty presented in the omnidirectional robot scenario is the \"piano movers\" problem, which requires the robot to turn and maneuver through narrow spaces.\n",
      "\n",
      "Q: What is the advantage of the proposed online replanning pipeline?\n",
      "A: The proposed online replanning pipeline allows the robot to create new, updated plans that take into account disturbances and help it navigate through various environments.\n",
      "\n",
      "Q: What is the proposed use of NeRF-based representation in collision-free navigation?\n",
      "A: The proposed use of NeRF-based representation is to allow robots to gain advantages in collision-free navigation.\n",
      "\n",
      "Q: What is the proposed trajectory optimization based on in the presented method?\n",
      "A: The proposed trajectory optimization is based on discrete time differential flatness dynamics.\n",
      "\n",
      "Q: How does the proposed vision-based state filter help in the proposed pipeline?\n",
      "A: The proposed vision-based state filter helps in the proposed pipeline by allowing for more accurate state estimation.\n",
      "\n",
      "Q: What is the anticipated outcome of using an iNeRF estimator initialized without a dynamics prior?\n",
      "A: The anticipated outcome of using an iNeRF estimator initialized without a dynamics prior is that it will quickly diverge.\n",
      "\n",
      "Q: How does the proposed full filter perform compared to the dynamically-informed iNeRF baseline?\n",
      "A: The proposed full filter outperforms the dynamically-informed iNeRF baseline on almost every metric and does not under-perform.\n",
      "\n",
      "Q: How does the proposed pipeline handle executed trajectories that incur a higher cost than the initial plan?\n",
      "A: The proposed pipeline handles executed trajectories that incur a higher cost than the initial plan by generating collision-free trajectories and allowing the robot to reach the goal.\n",
      "\n",
      "Q: What factors limit the execution speed of the proposed method?\n",
      "A: The execution speed of the proposed method is limited by the underlying NeRF representation.\n",
      "\n",
      "Q: What is ongoing work for improving the proposed pipeline?\n",
      "A: Ongoing work includes encouraging trajectories to point the camera in directions with greater gradient information and using the uncertainty metrics calculated by the state estimator to reduce collision risk.\n",
      "\n",
      "Q: Why is it important to improve the pixel sub-sampling heuristic employed by the state filter?\n",
      "A: It is important to improve the pixel sub-sampling heuristic employed by the state filter because it can improve the accuracy of the state estimation.\n",
      "\n",
      "Q: What is the final goal of the proposed method?\n",
      "A: The final goal of the proposed method is to implement it on real quadrotors in real scenes to demonstrate its performance beyond simulation.\n",
      "\n",
      "Q: What is pixelnerf?\n",
      "A: Pixelnerf is a neural radiance field model that uses one or very few images to generate 3D reconstructions. \n",
      "\n",
      "Q: What is Fastnerf?\n",
      "A: Fastnerf is a high-fidelity neural rendering technique capable of producing images at 200 frames per second. \n",
      "\n",
      "Q: What are Implicit Surface Representations?\n",
      "A: Implicit Surface Representations are used as layers in neural networks for the purpose of 3D shape representation. \n",
      "\n",
      "Q: What is DeepSDF?\n",
      "A: DeepSDF is a continuous signed distance function learning method used for 3D shape representation. \n",
      "\n",
      "Q: What are Occupancy Networks?\n",
      "A: Occupancy Networks refer to machine learning techniques that learn 3D reconstructions in function space. \n",
      "\n",
      "Q: What is iNeRF?\n",
      "A: iNeRF is a neural radiance field model that is designed to invert pixels to estimate pose. \n",
      "\n",
      "Q: What is Trajectory Optimization?\n",
      "A: Trajectory optimization involves optimizing the path of a robot or vehicle from one point to another, taking into account physical and environmental constraints. \n",
      "\n",
      "Q: What is CHOMP?\n",
      "A: CHOMP refers to gradient optimization techniques used for efficient motion planning. \n",
      "\n",
      "Q: What is Minimum Snap Trajectory Generation?\n",
      "A: Minimum Snap Trajectory Generation is a control technique used for quadrotors. \n",
      "\n",
      "Q: What is Tangent Space Backpropagation?\n",
      "A: Tangent Space Backpropagation is a neural network training algorithm that operates on 3D transformation groups.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "qa = ''\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "  completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "      {\"role\": \"user\", \"content\": convert_to_detokenized_text(chunks[i]) + \" \\n Using only the information provided above and not using the general knowledge of chatgpt, make 10 unique and truthful question and answer pairs using the format:\\nQ:\\nA:\"}\n",
    "    ]\n",
    "  )\n",
    "\n",
    "  print('\\n' + completion.choices[0].message.content)\n",
    "  qa += completion.choices[0].message.content + '\\n'\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 1\n",
      "Q: What is a Neural Radiance Field (NeRF)?\n",
      "A: NeRFs are deep-learned geometric representations that encode 3D geometry and color, enabling the efficient and photo-realistic simulation of scenes and objects.\n",
      "Question: 2\n",
      "Q: How can NeRFs be used for robot navigation?\n",
      "A: NeRFs can be used to represent the environment as a continuous density field, allowing for more efficient and gradient-based trajectory planning and motion optimization methods for robots using only RGB camera feedback.\n",
      "Question: 3\n",
      "Q: What advantages do NeRFs have over traditional environment representations like voxel grids or point clouds?\n",
      "A: NeRFs are trained directly on photographic images, making them more robust to environmental changes and easier to construct without precise feature extraction or alignment. They also encode the geometry as a continuous density field, enabling more efficient motion planning algorithms.\n",
      "Question: 4\n",
      "Q: How does the trajectory optimization algorithm work in this paper?\n",
      "A: The trajectory optimization algorithm is based on differential flatness and plans full, dynamically feasible trajectories that avoid collisions with high-density regions in the NeRF environment.\n",
      "Question: 5\n",
      "Q: What is the online replanning controller?\n",
      "A: The online replanning controller combines the trajectory planner with an optimization-based filter to estimate the robot's full dynamic state, and is used to navigate the robot through the environment in real-time using only onboard RGB camera feedback.\n",
      "Question: 6\n",
      "Q: What types of environments were used in the simulation results?\n",
      "A: The simulation results used custom-trained NeRF models of a playground, a church, and Stonehenge.\n",
      "Question: 7\n",
      "Q: What is the purpose of the pose filter in the navigation pipeline?\n",
      "A: The pose filter is used to estimate the robot's dynamic state given only onboard RGB camera feedback, and is used to update the trajectory planner and optimize the robot's control inputs in real-time.\n",
      "Question: 8\n",
      "Q: How is collision avoidance imposed in trajectory optimization for NeRF environments?\n",
      "A: Collision avoidance is imposed by constraining the SDF for all obstacles to be non-negative at all points on the robot body along the trajectory.\n",
      "Question: 9\n",
      "Q: How does this work differ from other vision-only navigation systems?\n",
      "A: This work focuses on using NeRFs as a geometric environment representation that can be used by any robot for navigation, and uses a modular approach to separate perception and control. It also specifically addresses the problem of collision avoidance in trajectory planning for NeRF environments.\n",
      "Question: 10\n",
      "Q: What is the main contribution of this paper?\n",
      "A: The main contribution of this paper is a vision-based navigation pipeline for a robot navigating through a 3D environment represented as a NeRF using only an onboard RGB camera for localization. This pipeline includes a new trajectory planning method, an optimization-based pose filter, and an online replanning controller.\n",
      "Question: 11\n",
      "Q: What is the representation used for the environment in this paper's navigation method?\n",
      "A: The navigation method uses a NeRF (Neural Radiance Fields) representation for the environment.\n",
      "Question: 12\n",
      "Q: What is the objective of the trajectory optimization in this paper's navigation method?\n",
      "A: The trajectory optimization seeks to minimize a combined objective of collision probability and control effort.\n",
      "Question: 13\n",
      "Q: How does the method in this paper leverage differential flatness to speed up planning?\n",
      "A: The method uses differential flatness, a property of some dynamical systems which allows their inputs and states to be represented using a smaller set of \"flat outputs\" and their derivatives, to speed up planning.\n",
      "Question: 14\n",
      "Q: What is the cost function used in the trajectory optimization of this paper's navigation method?\n",
      "A: The cost function used in the trajectory optimization seeks to minimize multi-objective cost given by collision penalty control penalty.\n",
      "Question: 15\n",
      "Q: What is the difference between CHOMP and the trajectory optimizer used in this paper's navigation method?\n",
      "A: CHOMP represents obstacle geometry by precomputing each obstacle's Signed Distance Field (SDF) on a finite grid, while the trajectory optimizer in this paper's navigation method uses a NeRF geometry representation that is continuous and of arbitrary resolution.\n",
      "Question: 16\n",
      "Q: What is the main challenge that prevents common trajectory planning methods from working with NeRF environment representations?\n",
      "A: Querying a NeRF at a point in space gives a density, not an absolute occupancy, which prevents the use of hard constraints.\n",
      "Question: 17\n",
      "Q: What is the approach used in this paper's navigation method to deal with the challenge of using NeRF environment representations?\n",
      "A: The paper's navigation method represents obstacles implicitly using the NeRF density and uses a set of flat output waypoints that are optimized to minimize a combined objective of collision probability and control effort.\n",
      "Question: 18\n",
      "Q: How is the method in this paper's navigation approach different from traditional planning pipelines for differentially flat systems?\n",
      "A: While prior methods only optimize the trajectory between static, hand-designed waypoints, this paper's navigation method uses a denser set of waypoints whose location can be optimized directly.\n",
      "Question: 19\n",
      "Q: What is the purpose of the state estimator in the proposed pipeline in this paper's navigation method?\n",
      "A: The state estimator uses a NeRF as a nonlinear measurement model to update the robot's belief about its current state after it takes an action and receives a noisy image from its onboard camera.\n",
      "Question: 20\n",
      "Q: What is the added benefit of the state estimation filter proposed in this paper's navigation method?\n",
      "A: The state estimation filter produces a state covariance which can be useful for other robotics algorithms running in parallel, such as collision avoidance with dynamic agents.\n",
      "Question: 21\n",
      "Q: What is the purpose of the proposed trajectory planner?\n",
      "A: The proposed trajectory planner is designed to generate aggressive, dynamically feasible trajectories for a robot that can avoid collisions in various environments.\n",
      "Question: 22\n",
      "Q: How does the state estimator improve the robot's performance in low photometric gradient regions?\n",
      "A: The state estimator relies more on the dynamics model to produce a more robust state estimation when the robot travels through areas with low photometric gradient information.\n",
      "Question: 23\n",
      "Q: What is the cost function used in the state estimation process?\n",
      "A: The cost function used in the state estimation process is a combination of the photometric loss and process loss.\n",
      "Question: 24\n",
      "Q: How does the algorithm reject outliers in the state estimation process?\n",
      "A: Outlier rejection is performed on the per-pixel loss to reduce variance in the state estimation process.\n",
      "Question: 25\n",
      "Q: What is the role of the NeRF model in the online replanning pipeline?\n",
      "A: The NeRF model is used to help the robot reason about collisions and generate updated plans during the online replanning process.\n",
      "Question: 26\n",
      "Q: How are trajectories optimized in the proposed trajectory planner?\n",
      "A: Trajectories are optimized using differential flatness to compute trajectories that pass through a series of waypoints, which are optimized based on the NeRF model.\n",
      "Question: 27\n",
      "Q: What is the purpose of the minimum-snap planner in the comparison study?\n",
      "A: The minimum-snap planner is used as a baseline comparison to the proposed trajectory planner, as both use differential flatness to compute trajectories.\n",
      "Question: 28\n",
      "Q: How is the performance of the proposed trajectory planner evaluated in the comparison study?\n",
      "A: The performance of the proposed trajectory planner is evaluated against the minimum-snap planner and a Rapidly-exploring Random Trees (RRT) method on 10 trajectories with varying obstacles and complexities in the Stonehenge environment.\n",
      "Question: 29\n",
      "Q: What is the difficulty presented in the omnidirectional robot scenario?\n",
      "A: The difficulty presented in the omnidirectional robot scenario is the \"piano movers\" problem, which requires the robot to turn and maneuver through narrow spaces.\n",
      "Question: 30\n",
      "Q: What is the advantage of the proposed online replanning pipeline?\n",
      "A: The proposed online replanning pipeline allows the robot to create new, updated plans that take into account disturbances and help it navigate through various environments.\n",
      "Question: 31\n",
      "Q: What is the proposed use of NeRF-based representation in collision-free navigation?\n",
      "A: The proposed use of NeRF-based representation is to allow robots to gain advantages in collision-free navigation.\n",
      "Question: 32\n",
      "Q: What is the proposed trajectory optimization based on in the presented method?\n",
      "A: The proposed trajectory optimization is based on discrete time differential flatness dynamics.\n",
      "Question: 33\n",
      "Q: How does the proposed vision-based state filter help in the proposed pipeline?\n",
      "A: The proposed vision-based state filter helps in the proposed pipeline by allowing for more accurate state estimation.\n",
      "Question: 34\n",
      "Q: What is the anticipated outcome of using an iNeRF estimator initialized without a dynamics prior?\n",
      "A: The anticipated outcome of using an iNeRF estimator initialized without a dynamics prior is that it will quickly diverge.\n",
      "Question: 35\n",
      "Q: How does the proposed full filter perform compared to the dynamically-informed iNeRF baseline?\n",
      "A: The proposed full filter outperforms the dynamically-informed iNeRF baseline on almost every metric and does not under-perform.\n",
      "Question: 36\n",
      "Q: How does the proposed pipeline handle executed trajectories that incur a higher cost than the initial plan?\n",
      "A: The proposed pipeline handles executed trajectories that incur a higher cost than the initial plan by generating collision-free trajectories and allowing the robot to reach the goal.\n",
      "Question: 37\n",
      "Q: What factors limit the execution speed of the proposed method?\n",
      "A: The execution speed of the proposed method is limited by the underlying NeRF representation.\n",
      "Question: 38\n",
      "Q: What is ongoing work for improving the proposed pipeline?\n",
      "A: Ongoing work includes encouraging trajectories to point the camera in directions with greater gradient information and using the uncertainty metrics calculated by the state estimator to reduce collision risk.\n",
      "Question: 39\n",
      "Q: Why is it important to improve the pixel sub-sampling heuristic employed by the state filter?\n",
      "A: It is important to improve the pixel sub-sampling heuristic employed by the state filter because it can improve the accuracy of the state estimation.\n",
      "Question: 40\n",
      "Q: What is the final goal of the proposed method?\n",
      "A: The final goal of the proposed method is to implement it on real quadrotors in real scenes to demonstrate its performance beyond simulation.\n",
      "Question: 41\n",
      "Q: What is pixelnerf?\n",
      "A: Pixelnerf is a neural radiance field model that uses one or very few images to generate 3D reconstructions. \n",
      "Question: 42\n",
      "Q: What is Fastnerf?\n",
      "A: Fastnerf is a high-fidelity neural rendering technique capable of producing images at 200 frames per second. \n",
      "Question: 43\n",
      "Q: What are Implicit Surface Representations?\n",
      "A: Implicit Surface Representations are used as layers in neural networks for the purpose of 3D shape representation. \n",
      "Question: 44\n",
      "Q: What is DeepSDF?\n",
      "A: DeepSDF is a continuous signed distance function learning method used for 3D shape representation. \n",
      "Question: 45\n",
      "Q: What are Occupancy Networks?\n",
      "A: Occupancy Networks refer to machine learning techniques that learn 3D reconstructions in function space. \n",
      "Question: 46\n",
      "Q: What is iNeRF?\n",
      "A: iNeRF is a neural radiance field model that is designed to invert pixels to estimate pose. \n",
      "Question: 47\n",
      "Q: What is Trajectory Optimization?\n",
      "A: Trajectory optimization involves optimizing the path of a robot or vehicle from one point to another, taking into account physical and environmental constraints. \n",
      "Question: 48\n",
      "Q: What is CHOMP?\n",
      "A: CHOMP refers to gradient optimization techniques used for efficient motion planning. \n",
      "Question: 49\n",
      "Q: What is Minimum Snap Trajectory Generation?\n",
      "A: Minimum Snap Trajectory Generation is a control technique used for quadrotors. \n",
      "Question: 50\n",
      "Q: What is Tangent Space Backpropagation?\n",
      "A: Tangent Space Backpropagation is a neural network training algorithm that operates on 3D transformation groups.\n"
     ]
    }
   ],
   "source": [
    "def remove_suffix(input_string, suffix):\n",
    "    if suffix and input_string.endswith(suffix):\n",
    "        return input_string[:-len(suffix)]\n",
    "    return input_string\n",
    "\n",
    "with  open('qa.txt' , 'w+') as f:\n",
    "    f.write(qa)\n",
    "    f.close()\n",
    "\n",
    "q , a = [] , []\n",
    "    \n",
    "with open('qa.txt' , 'r') as file:\n",
    "    for line in file:\n",
    "        if line[0] == 'Q': q.append(remove_suffix(line, '\\n'))\n",
    "        if line[0] == 'A': a.append(remove_suffix(line, '\\n'))\n",
    "        \n",
    "# print(q)\n",
    "# print(a)\n",
    "\n",
    "pairs = list(zip(q,a))\n",
    "# print(pairs)\n",
    "for i in range(len(pairs)):\n",
    "    print('Question: '+  str(i + 1))\n",
    "    print(pairs[i][0])\n",
    "    print(pairs[i][1])\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Found a total of 1 images in page 0\n",
      "[!] No images found on page 1\n",
      "[+] Found a total of 1 images in page 2\n",
      "[+] Found a total of 2 images in page 3\n",
      "[+] Found a total of 1 images in page 4\n",
      "[+] Found a total of 5 images in page 5\n",
      "[+] Found a total of 3 images in page 6\n",
      "[!] No images found on page 7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import io\n",
    "import fitz\n",
    "from PIL import Image\n",
    "\n",
    "file = FILE_PATH\n",
    "# open the file\n",
    "pdf_file = fitz.open(file)\n",
    "\n",
    "# iterate over pdf pages\n",
    "for page_index in range(len(pdf_file)):\n",
    "    # get the page itself\n",
    "    page = pdf_file[page_index]\n",
    "    image_list = page.get_images()\n",
    "    # printing number of images found in this page\n",
    "    if image_list:\n",
    "        print(f\"[+] Found a total of {len(image_list)} images in page {page_index}\")\n",
    "    else:\n",
    "        print(\"[!] No images found on page\", page_index)\n",
    "    for image_index, img in enumerate(page.get_images(), start=1):\n",
    "        # get the XREF of the image\n",
    "        xref = img[0]\n",
    "        # extract the image bytes\n",
    "        base_image = pdf_file.extract_image(xref)\n",
    "        image_bytes = base_image[\"image\"]\n",
    "        # get the image extension\n",
    "        image_ext = base_image[\"ext\"]\n",
    "        # load it to PIL\n",
    "        image = Image.open(io.BytesIO(image_bytes))\n",
    "        # save it to local disk\n",
    "        image.save(open(f\"images/image{page_index+1}_{image_index}.{image_ext}\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def preprocess_bytes(byte_string):\n",
    "    with open(byte_string, 'rb') as f:\n",
    "        img_array = np.asarray(bytearray(f.read()), dtype=\"uint8\")\n",
    "        img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
    "        img_resized = cv2.resize(img, dsize=(32, 32), interpolation=cv2.INTER_CUBIC)\n",
    "        return img_resized\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "img = tf.data.Dataset.list_files('images/*.png')\n",
    "\n",
    "\n",
    "img_arr = []\n",
    "\n",
    "img_iterator = img.as_numpy_iterator()\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        img_arr.append(preprocess_bytes(img_iterator.next()))\n",
    "    except:\n",
    "        break\n",
    "\n",
    "img_arr = np.array(img_arr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step\n",
      "['image1_1.jpeg', 'image3_1.png', 'image4_1.png', 'image4_2.png', 'image5_1.png', 'image6_1.jpeg', 'image6_2.png', 'image6_3.png', 'image6_4.png', 'image6_5.jpeg', 'image7_1.png', 'image7_2.png', 'image7_3.jpeg']\n",
      "9\n",
      "['image1_1.jpeg', 'image3_1.png', 'image4_1.png', 'image4_2.png', 'image5_1.png', 'image6_1.jpeg', 'image6_2.png', 'image6_3.png', 'image6_4.png']\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "file_path = f\"./modelsave/model2.h5\"\n",
    "model = keras.models.load_model(file_path)\n",
    "pred = model.predict(img_arr)\n",
    "\n",
    "ans = [[0,0,0]]*len(pred)\n",
    "for q in range(len(pred)):\n",
    "    for a in range(len(pred[q])):\n",
    "        # print(pred[q][a]/1 > 0.8)\n",
    "        if pred[q][a]/1 > 0.8: ans[q][a] =1\n",
    "        else: ans[q][a] = 0\n",
    "        \n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "onlyfiles = [f for f in listdir('images/')]\n",
    "print(onlyfiles)\n",
    "filtered = []\n",
    "idx = 0\n",
    "for a,b,c in ans:\n",
    "    if a  == 1 or b == 1: filtered.append(onlyfiles[idx])\n",
    "    idx +=1\n",
    "    \n",
    "print(len(filtered))\n",
    "print(filtered)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA not available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image1_1.jpeg\n",
      "image3_1.png\n",
      "image4_1.png\n",
      "image4_2.png\n",
      "image5_1.png\n",
      "image6_1.jpeg\n",
      "image6_2.png\n",
      "image6_3.png\n",
      "image6_4.png\n",
      "image6_5.jpeg\n",
      "image7_1.png\n",
      "image7_2.png\n",
      "image7_3.jpeg\n",
      "['processed/rectanglebox-{i}.jpg', 'processed/rectanglebox-{i}.jpg', 'processed/rectanglebox-{i}.jpg', 'processed/rectanglebox-{i}.jpg', 'processed/rectanglebox-{i}.jpg', 'processed/rectanglebox-{i}.jpg', 'processed/rectanglebox-{i}.jpg']\n"
     ]
    }
   ],
   "source": [
    "import cv2 \n",
    "import easyocr\n",
    "from datetime import datetime\n",
    "\n",
    "reader = easyocr.Reader(['en'], gpu = True)\n",
    "\n",
    "\n",
    "names = []\n",
    "for i in onlyfiles:\n",
    "    boxes = reader.readtext(f'images/{i}')\n",
    "    img = cv2.imread(f\"images/{i}\")\n",
    "    im2 = img.copy()\n",
    "    print(i)\n",
    "    for box in boxes:\n",
    "        x, y, w, h = int(box[0][0][0]), int(box[0][0][1]), int(box[0][1][0] - box[0][0][0]), int(box[0][0][1] - box[0][2][1]) \n",
    "        \n",
    "        # Draw the bounding box on the text area\n",
    "        rect=cv2.rectangle(im2, (x, y), (x + w, y - h), (0, 255, 0), 2)\n",
    "        \n",
    "        # Crop the bounding box area\n",
    "        cropped = im2[y:y - h, x:x + w]\n",
    "        \n",
    "        cv2.imwrite(f'processed/rectanglebox-{i}.jpg',rect)\n",
    "    if len(boxes) > 0: names.append('processed/rectanglebox-{i}.jpg')\n",
    "print(names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
